# pip install fastapi uvicorn boto3 torch torchaudio whisper openai pydub soundfile requests huggingface_hub pdfplumbe
# ffmpeg ë„ ì¸ìŠ¤í†¨í•´ì•¼ëŒ

from fastapi import FastAPI, HTTPException, BackgroundTasks
import os
import json
import whisper
import boto3
import torch
import time
import uuid
import openai
import torchaudio
import asyncio
import requests
from huggingface_hub import login
from pyannote.audio import Pipeline
from pydub import AudioSegment
import soundfile as sf
import json

with open("config.json", "r") as config_file:
    config = json.load(config_file)

app = FastAPI()

# # âœ… GPU ì„¤ì • (CUDA ê°€ëŠ¥í•˜ë©´ ì‚¬ìš©)
# device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# pipeline.to(device)
# âœ… GPU ì„¤ì • (ì—†ìœ¼ë©´ CPU ì‚¬ìš©)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = whisper.load_model("medium").to(device)

# ë¶„ì„ ê²°ê³¼ë¥¼ ë°±ì—”ë“œë¡œ ë°˜í™˜í•  ë•Œì˜ API
BACKEND_API_URL = "http://localhost:8080/api/ai/analyze/result"
# âœ… Hugging Face ë¡œê·¸ì¸
HUGGINGFACE_ACCESS_TOKEN = config["HUGGINGFACE_ACCESS_TOKEN"]
login(token=HUGGINGFACE_ACCESS_TOKEN)
# âœ… OpenAI API í‚¤ ì„¤ì • (GPT-4 ì‚¬ìš©)
OPENAI_API_KEY = config["OPENAI_API_KEY"]

# âœ… AWS S3 ì„¤ì • (IAM ì‚¬ìš©ì í‚¤ ì…ë ¥)
AWS_ACCESS_KEY = config["AWS_ACCESS_KEY"]
AWS_SECRET_KEY = config["AWS_SECRET_KEY"]
AWS_DEFAULT_REGION = config["AWS_DEFAULT_REGION"]
S3_BUCKET_NAME = config["S3_BUCKET_NAME"]  #ë²„ì¼“ ì£¼ì†Œ
s3_client = boto3.client('s3',
                      aws_access_key_id=AWS_ACCESS_KEY,
                      aws_secret_access_key=AWS_SECRET_KEY,
                      region_name=AWS_DEFAULT_REGION
                      )

# âœ… S3ì—ì„œ íŒŒì¼ ë‹¤ìš´ë¡œë“œí•˜ëŠ” í•¨ìˆ˜
async def download_from_s3(s3_url):
    """S3ì—ì„œ íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•˜ì—¬ ë¡œì»¬ì— ì €ì¥"""
    s3_key = s3_url.split(".amazonaws.com/")[-1]  # S3 ê°ì²´ í‚¤ ì¶”ì¶œ

    # âœ… ë‹¤ìš´ë¡œë“œí•  íŒŒì¼ì„ ì €ì¥í•  ë””ë ‰í† ë¦¬ ì„¤ì • (temp_audio í´ë” ìƒì„±)
    # download_dir = "temp_audio"

    # ê³ ìœ í•œ íŒŒì¼ëª… ìƒì„± (UUID ì‚¬ìš©)
    unique_filename = f"{uuid.uuid4()}.m4a"
    local_path = os.path.join("temp_audio", unique_filename)
    os.makedirs("temp_audio", exist_ok=True)    # ì—†ìœ¼ë©´ ë§Œë“¤ê¸°

    # ë¹„ë™ê¸° íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹¤í–‰
    await asyncio.to_thread(s3_client.download_file, S3_BUCKET_NAME, s3_key, local_path)
    # loop = asyncio.get_event_loop()
    # await loop.run_in_executor(None, lambda: s3_client.download_file(S3_BUCKET_NAME, s3_key, local_path))
    # âœ… íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹¤í–‰
    # s3_client.download_file(S3_BUCKET_NAME, s3_key, local_path)
    print(f"âœ… S3ì—ì„œ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {local_path}")

    return local_path  # ë‹¤ìš´ë¡œë“œëœ ë¡œì»¬ íŒŒì¼ ê²½ë¡œ ë°˜í™˜

# ğŸ”¹ Pyannote ìµœì‹  ëª¨ë¸ ë¡œë“œ
pipeline = Pipeline.from_pretrained(
    "pyannote/speaker-diarization-3.1",
    use_auth_token=HUGGINGFACE_ACCESS_TOKEN
)

# ğŸ“Œ ë¶„ì„í•  ìˆ˜ ìˆëŠ” íŒŒì¼ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
def ensure_wav_format(file_path):
    """ M4A ë˜ëŠ” WAV íŒŒì¼ì„ PCM 16-bit, 16kHz, Monoë¡œ ë³€í™˜ """
    if file_path.endswith(".m4a"):
        print("âš ï¸ M4A íŒŒì¼ì´ ê°ì§€ë¨. WAVë¡œ ë³€í™˜ ì¤‘...")
        file_path = convert_m4a_to_wav(file_path)  # M4A â†’ WAV ë³€í™˜

    try:
        # ğŸ” WAV íŒŒì¼ í™•ì¸
        with sf.SoundFile(file_path) as f:
            if f.format != 'WAV' or f.subtype != 'PCM_16' or f.samplerate != 16000 or f.channels != 1:
                print("âš ï¸ WAV íŒŒì¼ì´ PCM 16-bit, 16kHz, Mono í˜•ì‹ì´ ì•„ë‹˜. ë³€í™˜ í•„ìš”!")

                # âœ… WAVë¥¼ PCM 16-bit, 16kHz, Monoë¡œ ë³€í™˜
                audio = AudioSegment.from_wav(file_path)
                audio = audio.set_frame_rate(16000).set_channels(1).set_sample_width(2)
                
                converted_path = file_path.replace(".wav", "_fixed.wav")
                audio.export(converted_path, format="wav")

                print(f"ğŸ“¢ WAV íŒŒì¼ì„ ë³€í™˜í•˜ì—¬ ì €ì¥: {converted_path}")
                return converted_path  # ë³€í™˜ëœ íŒŒì¼ ê²½ë¡œ ë°˜í™˜

            else:
                print("âœ… WAV íŒŒì¼ì´ ì •ìƒì ì¸ PCM 16-bit, 16kHz, Mono í˜•ì‹ì…ë‹ˆë‹¤.")
                return file_path

    except Exception as e:
        print(f"âŒ WAV íŒŒì¼ ê²€ì‚¬ ì‹¤íŒ¨: {e}")
        return None

# m4a íŒŒì¼ì€ wavë¡œ ë³€í™˜
def convert_m4a_to_wav(m4a_path):
    """
    M4A íŒŒì¼ì„ WAV (16-bit PCM, 16kHz, Mono)ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
    """
    print(f"ğŸ“¢ M4A íŒŒì¼ ë³€í™˜ ì¤‘: {m4a_path}")

    # ğŸ”¹ M4A íŒŒì¼ ë¡œë“œ
    audio = AudioSegment.from_file(m4a_path, format="m4a")

    # ğŸ”¹ ë³€í™˜: 16kHz, Mono, 16-bit PCM
    audio = audio.set_frame_rate(16000).set_channels(1).set_sample_width(2)  # 2 bytes = 16-bit PCM

    # ğŸ”¹ ë³€í™˜ëœ WAV ì €ì¥
    wav_path = m4a_path.replace(".m4a", ".wav")
    audio.export(wav_path, format="wav")

    print(f"âœ… ë³€í™˜ ì™„ë£Œ: {wav_path}")
    return wav_path

# ğŸ“Œ ì˜¤ë””ì˜¤ íŒŒì¼ ì²˜ë¦¬
# async def process_audio(file_path):
#     """
#     Pyannoteë¡œ í™”ìë¥¼ ë¨¼ì € ë¶„ë¦¬í•œ í›„,
#     Whisperë¥¼ ì‚¬ìš©í•˜ì—¬ ê° í™”ìì˜ ìŒì„±ì„ ê°œë³„ì ìœ¼ë¡œ í…ìŠ¤íŠ¸ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
#     """
#     print("ğŸ“¢ Pyannoteë¡œ í™”ì ë¶„ë¦¬ ì¤‘...")

#     # âœ… ì˜¤ë””ì˜¤ íŒŒì¼ì„ ë¯¸ë¦¬ ë¡œë“œí•˜ì—¬ ì²˜ë¦¬ ì†ë„ í–¥ìƒ
#     waveform, sample_rate = torchaudio.load(file_path)

#     # âœ… Pyannote Speaker Diarization ì‹¤í–‰
#     diarization = pipeline(
#         {"waveform": waveform, "sample_rate": sample_rate},
#         min_speakers=1,  # ìµœì†Œ í™”ì ìˆ˜
#         max_speakers=3   # ìµœëŒ€ í™”ì ìˆ˜
#     )

#     # âœ… Whisper ëª¨ë¸ ë¡œë“œ
#     model = whisper.load_model("medium").to(device)

#     # âœ… Whisperë¡œ ì „ì²´ ì˜¤ë””ì˜¤ ë³€í™˜ (êµ¬ê°„ë³„ ë³€í™˜ í¬í•¨)
#     print("ğŸ“¢ Whisperë¡œ ìŒì„± ë³€í™˜ ì¤‘...")
#     result = model.transcribe(file_path, word_timestamps=True)

#     # âœ… Whisperì˜ ë³€í™˜ëœ êµ¬ê°„ë³„ ë°ì´í„° ì €ì¥
#     whisper_segments = [
#         {
#             "start": segment["start"],
#             "end": segment["end"],
#             "text": segment["text"]
#         }
#         for segment in result["segments"]
#     ]

#     # âœ… Pyannote ê²°ê³¼ì™€ Whisper ê²°ê³¼ ë§¤ì¹­
#     final_output = []
#     for whisper_segment in whisper_segments:
#         start_time, end_time = whisper_segment["start"], whisper_segment["end"]
#         matched_speaker = None

#         # Pyannoteì˜ í™”ì ì •ë³´ì™€ ì‹œê°„ ë²”ìœ„ ë¹„êµí•˜ì—¬ ë§¤ì¹­
#         for segment, _, speaker in diarization.itertracks(yield_label=True):
#             if segment.start <= start_time <= segment.end or segment.start <= end_time <= segment.end:
#                 matched_speaker = speaker
#                 break

#         final_output.append({
#             "speaker": matched_speaker if matched_speaker else "UNKNOWN",
#             "text": whisper_segment["text"],
#             "start_time": round(start_time, 2),
#             "end_time": round(end_time, 2)
#         })

#     print(f'ğŸ“¢ í™”ì ë¶„ë¦¬ + Whisper ë³€í™˜ ê²°ê³¼:\n{json.dumps(final_output, indent=4, ensure_ascii=False)}')
#     # print("í™”ì ë¶„ë¦¬ ì™„ë£Œ, STT ì™„ë£Œ")
#     return analyze_with_gpt(final_output)
async def process_audio(file_path):
    """
    Pyannoteë¡œ í™”ìë¥¼ ë¨¼ì € ë¶„ë¦¬í•œ í›„,
    Whisperë¥¼ ì‚¬ìš©í•˜ì—¬ ê° í™”ìì˜ ìŒì„±ì„ ê°œë³„ì ìœ¼ë¡œ í…ìŠ¤íŠ¸ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
    """
    print("ğŸ“¢ Pyannoteë¡œ í™”ì ë¶„ë¦¬ ì¤‘...")

    try:
        # âœ… ì˜¤ë””ì˜¤ íŒŒì¼ì„ ë¯¸ë¦¬ ë¡œë“œí•˜ì—¬ ì²˜ë¦¬ ì†ë„ í–¥ìƒ
        waveform, sample_rate = torchaudio.load(file_path)

        # âœ… Pyannote Speaker Diarization ì‹¤í–‰
        diarization = pipeline(
            {"waveform": waveform, "sample_rate": sample_rate},
            min_speakers=1,  # ìµœì†Œ í™”ì ìˆ˜
            max_speakers=3   # ìµœëŒ€ í™”ì ìˆ˜
        )

        # âœ… Whisper ëª¨ë¸ ë¡œë“œ
        model = whisper.load_model("medium").to(device)

        # âœ… Whisperë¡œ ì „ì²´ ì˜¤ë””ì˜¤ ë³€í™˜ (êµ¬ê°„ë³„ ë³€í™˜ í¬í•¨)
        print("ğŸ“¢ Whisperë¡œ ìŒì„± ë³€í™˜ ì¤‘...")
        result = model.transcribe(file_path, word_timestamps=True)

        if not result or "segments" not in result:
            print("âŒ Whisper ë³€í™˜ ì‹¤íŒ¨")
            return "FAILED"  # âœ… None ëŒ€ì‹  "FAILED" ë°˜í™˜

        whisper_segments = [
            {
                "start": segment["start"],
                "end": segment["end"],
                "text": segment["text"]
            }
            for segment in result["segments"]
        ]

        final_output = analyze_with_gpt(whisper_segments)  # âœ… GPT ë¶„ì„ ì‹¤í–‰

        if not final_output:
            print("âŒ GPT ë¶„ì„ ì‹¤íŒ¨")
            return "FAILED"

        return final_output  # âœ… ì˜¬ë°”ë¥¸ ê²°ê³¼ ë°˜í™˜

    except Exception as e:
        print(f"âŒ STT ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return "FAILED"  # âœ… ì˜ˆì™¸ ë°œìƒ ì‹œ "FAILED" ë°˜í™˜
    

# ğŸ“Œ AI ë¶„ì„: ìƒë‹´ ë‚´ìš© ìš”ì•½ ë° ì •ë¦¬
def analyze_with_gpt(text_data):
    """
    AI (GPT-4o-mini)ë¥¼ ì´ìš©í•´ ìƒë‹´ ë‚´ìš©ì„ ë¶„ì„í•˜ê³  ìš”ì•½í•˜ëŠ” í•¨ìˆ˜
    """
    print("ğŸ“¢ AI ë¶„ì„ ì§„í–‰ ì¤‘...")

    prompt = f"""
        ë‹¤ìŒ ì›¨ë”©í™€ ìƒë‹´ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬, í‘œ í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•´ì¤˜.
        
        1. ìƒë‹´ ë‚´ìš©ì„ ê°„ëµíˆ ìš”ì•½í•˜ê³ , ì–´ë–¤ ìƒí™©ì¸ì§€ ì„¤ëª…í•´.
        2. ì£¼ìš” í•­ëª©ì„ ì •ë¦¬í•˜ì—¬ JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•´.
        3. ë‹¤ë¥¸ ë‹µë³€ì€ ì¶”ê°€í•˜ì§€ ë§ê³ , ì˜¤ì§ JSON í˜•ì‹ë§Œ ë‹µë³€ì— ë‹´ì•„.
        4. 1ê°œì˜ JSON ê°ì²´ì—ëŠ” ì˜¤ì§ 1ê°œì˜ ë‚ ì§œì™€ í™€ë§Œ ì¡´ì¬í•´ì•¼ í•´. 
        5. ë§Œì•½ ìƒë‹´ ë‚´ìš©ì— ì—¬ëŸ¬ ë‚ ì§œì™€ ì—¬ëŸ¬ í™€ì´ ì¡´ì¬í•œë‹¤ë©´, ëª¨ë‘ ê°ê°ì˜ JSON ê°ì²´ë¡œ ì‘ì„±í•´ ì—¬ëŸ¬ ê°œì˜ JSON ê°ì²´ë¡œ ë‹µë³€í•´. 
        6. JSON í•„ë“œëŠ” ì•„ë˜ í˜•ì‹ì„ ë”°ë¥¼ ê²ƒ:
        
        ```json
        {{
            "ë‚ ì§œ": "25/03/22 14:00",
            "ì—…ì²´ëª…": "ì›¨ìŠ¤í„´ë² ë‹ˆë¹„ìŠ¤",
            "í™€ëª…" : "ê·¸ëœë“œë³¼ë¥¨",
            "ìœ„ì¹˜": "ì„œìš¸ ê°•ë‚¨êµ¬",
            "ê°€ëŠ¥ ë‚ ì§œ": "2026ë…„ 2ì›” 28ì¼ ë¶ˆê°€ëŠ¥", 
            "í™€_ìœ í˜•": "ì»¤í‹°ì§€í™€",
            "ìµœëŒ€ì¸ì›": "300ëª…"
            "ëŒ€ê´€ë£Œ": "200ë§Œ ì›",
            "ì˜ˆì‹_ë°©ì‹": "ë‹¨ë…í™€",
            "ì‹ì‚¬_í˜•íƒœ": "ë·”í˜",
            "1ì¸ë‹¹_ì‹ì‚¬_ë¹„ìš©": "5ë§Œì›",
            "ìµœì†Œ_ë³´ì¥_ì¸ì›": "200ëª…",
            "í¬í•¨_ì„œë¹„ìŠ¤": ["í”Œë¼ì›Œ ì¥ì‹", "ì¡°ëª… ì—°ì¶œ"],
            "ê³„ì•½ê¸ˆ": "50ë§Œ ì›",
            "í™˜ë¶ˆ_ê·œì •": "1ê°œì›” ì „ 100% í™˜ë¶ˆ, 2ì£¼ ì „ 50% í™˜ë¶ˆ",
            "ê¸°íƒ€ì‚¬í•­" : ["ì£¼ì°¨ 100ëŒ€ ë¬´ë£Œ", "í‰ì¼ ì˜ˆì‹ 10% í• ì¸","3ê°œì›” ë‚´ ê³„ì•½ì‹œ ëŒ€ê´€ë£Œ 10% í• ì¸ì¸"],
            }},
            {{
            "ë‚ ì§œ": "25/03/24 15:00",
            "ì—…ì²´ëª…": "ì›¨ìŠ¤í„´ë² ë‹ˆë¹„ìŠ¤",
            "í™€ëª…" : "ë¥´ë¯¸ì—˜ì›¨ë”©í™€í™€",
            "ìœ„ì¹˜": "ì„œìš¸ ê°•ë‚¨êµ¬",
            "ê°€ëŠ¥ ë‚ ì§œ": "2026ë…„ 2ì›” 27ì¼ ê°€ëŠ¥", 
            "í™€_ìœ í˜•": "ì±”í”Œí”Œí™€",
            "ìµœëŒ€ì¸ì›": "240ëª…"
            "ëŒ€ê´€ë£Œ": "1500ë§Œ ì›",
            "ì˜ˆì‹_ë°©ì‹": "ë‹¨ë…í™€",
            "ì‹ì‚¬_í˜•íƒœ": "ë·”í˜",
            "1ì¸ë‹¹_ì‹ì‚¬_ë¹„ìš©": "5ë§Œì›ì›",
            "ìµœì†Œ_ë³´ì¥_ì¸ì›": "100ëª…",
            "í¬í•¨_ì„œë¹„ìŠ¤": ["í”Œë¼ì›Œ ì¥ì‹", "í”¼ì•„ë…¸ 4ì¤‘ì£¼","ì‚¬íšŒì"],
            "ê³„ì•½ê¸ˆ": "30ë§Œ ì›",
            "í™˜ë¶ˆ_ê·œì •": "1ê°œì›” ì „ 100% í™˜ë¶ˆ, 2ì£¼ ì „ 50% í™˜ë¶ˆ",
            "ê¸°íƒ€ì‚¬í•­" : ["ì£¼ì°¨ 100ëŒ€ ë¬´ë£Œ", "í‰ì¼ ì˜ˆì‹ 10% í• ì¸","3ê°œì›” ë‚´ ê³„ì•½ì‹œ ëŒ€ê´€ë£Œ 10% í• ì¸ì¸"],
        }}
        ```

        4. ìƒë‹´ ë‚´ìš©ì—ì„œ í•´ë‹¹ ì •ë³´ê°€ ëˆ„ë½ëœ ê²½ìš°, "ì •ë³´ ì—†ìŒ"ìœ¼ë¡œ í‘œê¸°í•´.
        5. ë°ì´í„° ë¶„ì„ì„ ìœ„í•´ ê¹”ë”í•œ JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•´.

        --- ìƒë‹´ ë‚´ìš© ---
        
        {text_data}
    """

    client = openai.Client(api_key=OPENAI_API_KEY)

    for attempt in range(5):
        try:
            response = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "ë„ˆëŠ” ì›¨ë”© ìƒë‹´ ë‚´ìš©ì„ ì •ë¦¬í•˜ëŠ” AIì•¼."},
                    {"role": "user", "content": prompt}
                ]
            )

            if response and response.choices:
                return response.choices[0].message.content
            else:
                print("âŒ OpenAI API ì‘ë‹µì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
                return None

        except openai.RateLimitError:
            wait_time = (2 ** attempt)  # 2, 4, 8, 16ì´ˆ ëŒ€ê¸°
            print(f"âš ï¸ OpenAI ìš”ì²­ ì œí•œ ì´ˆê³¼. {wait_time}ì´ˆ í›„ ì¬ì‹œë„...")
            time.sleep(wait_time)

    print("âŒ 5íšŒ ì¬ì‹œë„ í›„ì—ë„ ì‹¤íŒ¨. API ì‚¬ìš©ëŸ‰ì„ í™•ì¸í•˜ì„¸ìš”.")
    return None

# ##################í…ŒìŠ¤íŠ¸###################
# @app.post("/predict")
# def summarize_audio_request(data: dict):
#     analysis_result = analyze_with_gpt("ì•ˆë…•í•˜ì„¸ìš”ìš”")
#     audio_summary_id = data.get("audio_summary_id")
#     send_to_backend(audio_summary_id, analysis_result, "SUCCESS")



@app.post("/predict")
async def summarize_audio_request(data: dict, background_tasks: BackgroundTasks):
    file_url = data.get("file_url")
    # request_id = str(uuid.uuid4())  # ê³ ìœ í•œ ìš”ì²­ ID ìƒì„±
    audio_summary_id = data.get("audio_summary_id")

    if not file_url:
        raise HTTPException(status_code=400, detail="íŒŒì¼ URLì´ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    print(f"ğŸ“¢ S3ì—ì„œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì¤‘: {file_url}")

    # âœ… 1. S3ì—ì„œ ë‹¤ìš´ë¡œë“œí•˜ì—¬ ë¡œì»¬ íŒŒì¼ë¡œ ì €ì¥
    local_file_path = await download_from_s3(file_url)

    # âœ… 2. ë³€í™˜ (M4A â†’ WAV)
    converted_file = ensure_wav_format(local_file_path)
        
    if not converted_file:
        return {"status": "ERROR", "message": "íŒŒì¼ ë³€í™˜ ì‹¤íŒ¨"}
    
     # âœ… 3. STT ë¶„ì„ì„ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰
    background_tasks.add_task(process_and_send_result, converted_file,audio_summary_id)

    return {"status": "PROCESSING", "message": "íŒŒì¼ì´ ì²˜ë¦¬ ì¤‘ì…ë‹ˆë‹¤. ë‚˜ì¤‘ì— ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”."}  
    # return(process_file(converted_file))  # ë³€í™˜ëœ WAV íŒŒì¼ë¡œ ì‹¤í–‰

# âœ… STT ë¶„ì„ & ë°±ì—”ë“œ ì „ì†¡ (ë¹„ë™ê¸°)
async def process_and_send_result(file_path, audio_summary_id):
    """STT ë³€í™˜ í›„ ê²°ê³¼ë¥¼ ë°±ì—”ë“œë¡œ ì „ì†¡í•˜ëŠ” ë¹„ë™ê¸° í•¨ìˆ˜"""
    analysis_result = await process_audio(file_path)

    # if analysis_result:
    #     await send_to_backend(audio_summary_id, analysis_result, "COMPLETED")
    # else:
    #     print(f"âŒ STT ë³€í™˜ ì‹¤íŒ¨: {audio_summary_id}")
    #     await send_to_backend(audio_summary_id,"" ,"FAILED")
    # âœ… None ë°©ì§€: analysis_resultê°€ "FAILED"ì¸ ê²½ìš° ë¹ˆ ë¬¸ìì—´ë¡œ ì„¤ì •
    if not analysis_result or analysis_result == "FAILED":
        print(f"âŒ STT ë³€í™˜ ì‹¤íŒ¨: {audio_summary_id}")
        analysis_result = ""

    # âœ… send_to_backend()ëŠ” í•­ìƒ ìœ íš¨í•œ ê°’ ì „ë‹¬
    await send_to_backend(audio_summary_id, analysis_result, "COMPLETED" if analysis_result else "FAILED")


# âœ… ë°±ì—”ë“œë¡œ ê²°ê³¼ ì „ì†¡ (ë¹„ë™ê¸°)
async def send_to_backend(audio_summary_id, analysis_result, status):
    """Spring Boot ë°±ì—”ë“œë¡œ ë¶„ì„ ê²°ê³¼ ì „ì†¡"""
    # âœ… None ë°©ì§€: ë¹ˆ ë¬¸ìì—´ë¡œ ëŒ€ì²´
    if analysis_result is None:
        analysis_result = ""

    payload = {"audioSummaryId": audio_summary_id, "summaryResult": analysis_result, "status": status}
    response = requests.patch(BACKEND_API_URL, json=payload)
    
    print(f"ğŸ“¢ ë°±ì—”ë“œ ì‘ë‹µ ì½”ë“œ: {response.status_code}, ì‘ë‹µ: {response.text}")
    if response.status_code == 200:
        print(f"âœ… [ë°±ì—”ë“œ ì „ì†¡ ì™„ë£Œ] ìš”ì²­ ID: {audio_summary_id}")
    else:
        print(f"âŒ [ë°±ì—”ë“œ ì‘ë‹µ ì˜¤ë¥˜] ìš”ì²­ ID: {audio_summary_id}, ì˜¤ë¥˜: {response.text}")